import pandas as pd
import numpy as np
import torch
from torch import nn
from torch_geometric.data import Data
from torch_geometric.nn import TransformerConv, global_mean_pool
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# =====================================================
# 1ï¸âƒ£ Äá»ŒC Dá»® LIá»†U
# =====================================================
DATA_PATH = "NF-ToN-IoT-v2-train.csv"   # hoáº·c NF-ToN-IoT-v2-train.csv
df = pd.read_csv(DATA_PATH, nrows=50000)  # Ä‘á»c demo 50k dÃ²ng

# XÃ¡c Ä‘á»‹nh cá»™t nhÃ£n
label_col = [c for c in df.columns if "label" in c.lower() or "attack" in c.lower()]
if len(label_col) == 0:
    raise ValueError("âŒ KhÃ´ng tÃ¬m tháº¥y cá»™t nhÃ£n. HÃ£y kiá»ƒm tra tÃªn cá»™t trong file.")
df = df.rename(columns={label_col[0]: "label"})

# Encode label
le = LabelEncoder()
df["y"] = le.fit_transform(df["label"].astype(str))
print(f"ğŸŸ¢ ÄÃ£ mÃ£ hÃ³a {len(le.classes_)} lá»›p:", le.classes_)

# =====================================================
# 2ï¸âƒ£ LÃ€M Sáº CH Dá»® LIá»†U Sá»
# =====================================================
feature_cols = [c for c in df.columns if c not in ["label", "y"]]
X = df[feature_cols].copy()

# Ã‰p toÃ n bá»™ vá» dáº¡ng sá»‘
for c in X.columns:
    X[c] = pd.to_numeric(X[c], errors='coerce')

# Thay tháº¿ NaN, Inf, giÃ¡ trá»‹ cá»±c lá»›n
X = X.replace([np.inf, -np.inf, np.nan], 0)
X = np.clip(X, -1e9, 1e9)
X = X.astype(np.float32)

# Chuáº©n hÃ³a
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Tensor hÃ³a
X_tensor = torch.tensor(X_scaled, dtype=torch.float)
y_tensor = torch.tensor(df["y"].values, dtype=torch.long)

num_nodes = X_tensor.shape[0]
num_features = X_tensor.shape[1]
num_classes = len(le.classes_)
print(f"âœ… Node: {num_nodes} | Feature: {num_features} | Lá»›p: {num_classes}")

# =====================================================
# 3ï¸âƒ£ Táº O EDGE GIáº¢ Láº¬P Dá»°A TRÃŠN SIMILARITY
# =====================================================
from sklearn.neighbors import NearestNeighbors

k = 5  # sá»‘ lÃ¡ng giá»ng
nbrs = NearestNeighbors(n_neighbors=k + 1, algorithm='auto').fit(X_scaled)
distances, indices = nbrs.kneighbors(X_scaled)

edge_index = []
for i in range(num_nodes):
    for j in indices[i][1:]:  # bá» chÃ­nh nÃ³
        edge_index.append([i, j])

edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()

# =====================================================
# 4ï¸âƒ£ Táº O GRAPH DATA
# =====================================================
data = Data(x=X_tensor, edge_index=edge_index, y=y_tensor)

# Train/Test Split
train_mask, test_mask = train_test_split(np.arange(num_nodes), test_size=0.2, random_state=42)
data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)
data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)
data.train_mask[train_mask] = True
data.test_mask[test_mask] = True

print(f"ğŸ§© Train nodes: {data.train_mask.sum()} | Test nodes: {data.test_mask.sum()}")

# =====================================================
# 5ï¸âƒ£ MÃ” HÃŒNH GRAPH TRANSFORMER
# =====================================================
class GraphTransformer(nn.Module):
    def __init__(self, in_channels, hidden_channels, num_classes, heads=4, dropout=0.3):
        super().__init__()
        self.conv1 = TransformerConv(in_channels, hidden_channels, heads=heads, dropout=dropout)
        self.conv2 = TransformerConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout)
        self.lin = nn.Linear(hidden_channels * heads, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index, batch=None):
        x = self.conv1(x, edge_index)
        x = torch.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        x = torch.relu(x)
        x = self.dropout(x)
        x = self.lin(x)
        return x

# =====================================================
# 6ï¸âƒ£ TRAIN LOOP
# =====================================================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GraphTransformer(num_features, 64, num_classes).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)
criterion = nn.CrossEntropyLoss()

data = data.to(device)

for epoch in range(1, 11):  # train 10 epoch demo
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)
    loss = criterion(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

    # Evaluate
    model.eval()
    pred = out.argmax(dim=1)
    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()
    acc = correct / data.test_mask.sum().item()

    print(f"Epoch {epoch:02d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f}")

# =====================================================
# 7ï¸âƒ£ ÄÃNH GIÃ CUá»I CÃ™NG
# =====================================================
model.eval()
pred = out.argmax(dim=1).cpu().numpy()
true = data.y.cpu().numpy()

print("\nğŸ“Š Classification Report:")
print(classification_report(true[data.test_mask.cpu().numpy()],
                            pred[data.test_mask.cpu().numpy()],
                            target_names=le.classes_))





#####################################
import pandas as pd
import numpy as np
import torch
from torch import nn
from torch_geometric.data import Data
from torch_geometric.nn import HypergraphConv, global_mean_pool
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.neighbors import NearestNeighbors

# =====================================================
# 1ï¸âƒ£ Äá»ŒC VÃ€ Xá»¬ LÃ Dá»® LIá»†U
# =====================================================
DATA_PATH = "NF-ToN-IoT-v2.csv"
df = pd.read_csv(DATA_PATH, nrows=50000)

label_col = [c for c in df.columns if "label" in c.lower() or "attack" in c.lower()]
if len(label_col) == 0:
    raise ValueError("âŒ KhÃ´ng tÃ¬m tháº¥y cá»™t nhÃ£n.")
df = df.rename(columns={label_col[0]: "label"})

le = LabelEncoder()
df["y"] = le.fit_transform(df["label"].astype(str))

feature_cols = [c for c in df.columns if c not in ["label", "y"]]
X = df[feature_cols].copy()

# LÃ m sáº¡ch dá»¯ liá»‡u
for c in X.columns:
    X[c] = pd.to_numeric(X[c], errors='coerce')
X = X.replace([np.inf, -np.inf, np.nan], 0)
X = np.clip(X, -1e9, 1e9).astype(np.float32)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_tensor = torch.tensor(X_scaled, dtype=torch.float)
y_tensor = torch.tensor(df["y"].values, dtype=torch.long)

num_nodes = X_tensor.shape[0]
num_features = X_tensor.shape[1]
num_classes = len(le.classes_)
print(f"âœ… Node: {num_nodes} | Feature: {num_features} | Lá»›p: {num_classes}")

# =====================================================
# 2ï¸âƒ£ Táº O HYPEREDGE (má»—i hyperedge ná»‘i k node gáº§n nhau)
# =====================================================
k = 5
nbrs = NearestNeighbors(n_neighbors=k+1).fit(X_scaled)
_, indices = nbrs.kneighbors(X_scaled)

# edge_index_hyper: (2, num_hyperedges*k)
# Má»—i node i sáº½ cÃ³ 1 hyperedge id = i
hyperedges = []
for i in range(num_nodes):
    for j in indices[i][1:]:
        hyperedges.append([i, j])
hyperedges = torch.tensor(hyperedges, dtype=torch.long).t().contiguous()

# =====================================================
# 3ï¸âƒ£ Äá»ŠNH NGHÄ¨A MÃ” HÃŒNH HYPERGRAPH
# =====================================================
class HyperGraphNet(nn.Module):
    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.3):
        super().__init__()
        self.hconv1 = HypergraphConv(in_channels, hidden_channels)
        self.hconv2 = HypergraphConv(hidden_channels, hidden_channels)
        self.lin = nn.Linear(hidden_channels, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index):
        x = torch.relu(self.hconv1(x, edge_index))
        x = self.dropout(x)
        x = torch.relu(self.hconv2(x, edge_index))
        x = self.dropout(x)
        x = self.lin(x)
        return x

# =====================================================
# 4ï¸âƒ£ TRAIN / TEST SPLIT
# =====================================================
train_mask, test_mask = train_test_split(np.arange(num_nodes), test_size=0.2, random_state=42)
data = Data(x=X_tensor, edge_index=hyperedges, y=y_tensor)
data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)
data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)
data.train_mask[train_mask] = True
data.test_mask[test_mask] = True

# =====================================================
# 5ï¸âƒ£ TRAINING LOOP
# =====================================================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = HyperGraphNet(num_features, 64, num_classes).to(device)
data = data.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)
criterion = nn.CrossEntropyLoss()

for epoch in range(1, 11):
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)
    loss = criterion(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

    model.eval()
    pred = out.argmax(dim=1)
    acc = (pred[data.test_mask] == data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()
    print(f"Epoch {epoch:02d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f}")

# =====================================================
# 6ï¸âƒ£ BÃO CÃO Káº¾T QUáº¢
# =====================================================
model.eval()
pred = out.argmax(dim=1).cpu().numpy()
true = data.y.cpu().numpy()
print("\nğŸ“Š Classification Report:")
print(classification_report(true[data.test_mask.cpu().numpy()],
                            pred[data.test_mask.cpu().numpy()],
                            target_names=le.classes_))




#########################

1. Hiá»ƒu vá» Fecograph Paper

a) Má»¥c tiÃªu cá»§a FeCoGraph:
XÃ¢y dá»±ng mÃ´ hÃ¬nh Federated Graph Contrastive Learning cho Network Intrusion Detection (NIDS) trong bá»‘i cáº£nh few-shot (Ã­t dá»¯ liá»‡u táº¥n cÃ´ng).

b) cÃ¡c luá»“ng

Data => line graph => encoder (dÃ¹ng Graph Transformer) => Constrative and classification

b) giáº£i phÃ¡p
Encoder cá»§a FeCoGraph ThÆ°á»ng lÃ  Graph Neural Network (GCN hoáº·c GAT) â†’ há»c embedding cho cÃ¡c node (thiáº¿t bá»‹ hoáº·c flow máº¡ng).
=> Váº¥n Ä‘á»: GCN hoáº·c GAT chá»‰ truyá»n thÃ´ng tin cá»¥c bá»™ (local neighborhood) â†’ khÃ³ náº¯m má»‘i liÃªn há»‡ toÃ n cá»¥c trong máº¡ng IoT â†’ Graph Transformer cÃ³ thá»ƒ thay tháº¿ Ä‘á»ƒ cáº£i thiá»‡n.


2. Graph Transformer

- Nguá»“n gá»‘c: xuáº¥t phÃ¡t tá»« Transformer (self-attention) trong NLP, má»Ÿ rá»™ng Ä‘á»ƒ Ã¡p dá»¥ng cho Ä‘á»“ thá»‹.
- CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng: Thay vÃ¬ â€œtá»«â€ â†’ attention toÃ n bá»™ â€œnodeâ€. vÃ  nÃ³ DÃ¹ng attention weight cÃ³ bias theo thÃ´ng tin cáº¡nh (edge) hoáº·c vá»‹ trÃ­ trong graph.

- Lá»£i tháº¿:
+ Há»c Ä‘Æ°á»£c má»‘i quan há»‡ dÃ i háº¡n giá»¯a cÃ¡c node, khÃ´ng chá»‰ lÃ¡ng giá»ng.
+ Giá»¯ Ä‘Æ°á»£c cáº¥u trÃºc Ä‘á»“ thá»‹ phá»©c táº¡p.

- VÃ­ dá»¥ cá»¥ thá»ƒ:
+ Graphormer (Microsoft, 2021): dÃ¹ng distance encoding vÃ  edge encoding.
+ Trong PyTorch Geometric: lá»›p TransformerConv(in_channels, out_channels, heads=8) cÃ³ thá»ƒ thay GCNConv.

3. Kiáº¿n truc Graph Transformer

CÃ³ 2 khá»‘i TransformerConv â†’ má»—i khá»‘i tÆ°Æ¡ng Ä‘Æ°Æ¡ng 1 layer attention-based graph block. Má»—i block Ä‘á»u cÃ³: Multi-head Graph Attention => Message passing (tá»« cÃ¡c node lÃ¡ng giá»ng) (Edge-based message passing) => Dropout + ReLU

Sau Ä‘Ã³ lÃ  1 Linear layer (classifier).



